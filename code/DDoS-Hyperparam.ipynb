{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import ogb\n",
    "from tqdm import tqdm\n",
    "import hiplot as hip\n",
    "from copy import deepcopy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import Subset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aliciapliego/Projects/drug_synergy/AI-predict/code\n",
      "/Users/aliciapliego/Projects/drug_synergy/AI-predict\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "cwd_parent = os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "print(cwd_parent)\n",
    "\n",
    "sys.path.append(cwd_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepadr\n",
    "from deepadr.dataset import *\n",
    "from deepadr.utilities import *\n",
    "from deepadr.chemfeatures import *\n",
    "from deepadr.train_functions import *\n",
    "from deepadr.model_gnn_ogb import GNN, DeepAdr_SiameseTrf, ExpressionNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata_dir = '../data/raw/'\n",
    "processed_dir = '../data/processed/'\n",
    "up_dir = '..'\n",
    "targetdata_dir_raw = '/Users/aliciapliego/Projects/drug_synergy/AI-predict/data/processed/dataset_generation/raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no GPU devices available!!\n"
     ]
    }
   ],
   "source": [
    "report_available_cuda_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gpu = torch.cuda.device_count()\n",
    "n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_cpu = get_device(to_gpu=False)\n",
    "# device_gpu = get_device(True, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.13.1\n",
      "CUDA: None\n",
      "3.9.13 (main, May 24 2022, 21:28:44) \n",
      "[Clang 13.0.0 (clang-1300.0.29.30)]\n"
     ]
    }
   ],
   "source": [
    "print(\"torch:\", torch.__version__)\n",
    "print(\"CUDA:\", torch.version.cuda)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options: \n",
    "# 'total_thresh' + 4,3,2\n",
    "# 'loewe_thresh', 'hsa_thresh', 'bliss_thresh', 'zip_thresh' + 1\n",
    "\n",
    "#score = 'loewe_thresh'\n",
    "#score_val = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DSdataset_name = f'DrugComb_{score}_{score_val}'\n",
    "\n",
    "#data_fname = 'data_v1' # v2 for baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DSdataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/aliciapliego/Projects/drug_synergy/AI-predict/data/processed/dataset_generation/experiments\n"
     ]
    }
   ],
   "source": [
    "#targetdata_dir = create_directory(os.path.join(processed_dir, DSdataset_name, data_fname))\n",
    "#targetdata_dir_raw = create_directory(os.path.join(targetdata_dir, \"raw\"))\n",
    "#targetdata_dir_processed = create_directory(os.path.join(targetdata_dir, \"processed\"))\n",
    "targetdata_dir_exp = \"/Users/aliciapliego/Projects/drug_synergy/AI-predict/data/processed/dataset_generation/experiments\"\n",
    "# # ReaderWriter.dump_data(dpartitions, os.path.join(targetdata_dir, 'data_partitions.pkl'))\n",
    "print(targetdata_dir_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "exp_dirs = glob.glob(targetdata_dir_exp+\"/hyp/exp_*\")\n",
    "len(exp_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetdata_dir_raw = os.path.abspath( \"/Users/aliciapliego/Projects/drug_synergy/AI-predict/data/processed/dataset_generation/raw\")\n",
    "targetdata_dir_processed = os.path.abspath(\"/Users/aliciapliego/Projects/drug_synergy/AI-predict/data/processed/dataset_generation/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_current_dir /Users/aliciapliego/Projects/drug_synergy/AI-predict/deepadr\n",
      "CPU times: user 319 µs, sys: 131 µs, total: 450 µs\n",
      "Wall time: 391 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Make sure to first run the \"DDoS_Dataset_Generation\" notebook first\n",
    "targetdata_dir = create_directory(os.path.join(processed_dir, 'dataset_generation'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.67 ms, sys: 540 ms, total: 543 ms\n",
      "Wall time: 544 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = MoleculeDataset(root=targetdata_dir, dataset='tdcSynergy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: MoleculeDataset(233326):\n",
      "====================\n",
      "Number of graphs: 233326\n",
      "Number of features: 9\n",
      "Number of classes: 4\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used_dataset = dataset\n",
    "\n",
    "# If you want to use a smaller subset of the dataset for testing\n",
    "smaller_dataset_len = int(len(dataset)/1)\n",
    "used_dataset = dataset[:smaller_dataset_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_stratified partitions function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = used_dataset.data.y[:smaller_dataset_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds=5\n",
    "valid_set_portion=0.1\n",
    "random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf_trte = StratifiedKFold(n_splits=num_folds, random_state=random_state, shuffle=True)  # split train and test\n",
    "skf_trte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf_trv = StratifiedShuffleSplit(n_splits=2, \n",
    "                                     test_size=valid_set_portion, \n",
    "                                     random_state=random_state)  # split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_partitions = {}\n",
    "X = np.zeros(len(y))\n",
    "fold_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test  = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "#Split one more time the train data (I can split it in the same ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in skf_trte.split(X,y):\n",
    "\n",
    "    x_tr = np.zeros(len(train_index))\n",
    "    y_tr = y[train_index]\n",
    "\n",
    "    for tr_index, val_index in skf_trv.split(x_tr, y_tr):\n",
    "        tr_ids = train_index[tr_index]\n",
    "        val_ids = train_index[val_index]\n",
    "        data_partitions[fold_num] = {'train': tr_ids,\n",
    "                                     'validation': val_ids,\n",
    "                                     'test': test_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "        print(\"fold_num:\", fold_num)\n",
    "        print('train data')\n",
    "        report_label_distrib(y[tr_ids])\n",
    "        print('validation data')\n",
    "        report_label_distrib(y[val_ids])\n",
    "        print('test data')\n",
    "        report_label_distrib(y[test_index])\n",
    "        print()\n",
    "        fold_num += 1\n",
    "        print(\"-\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fold_partitions = get_stratified_partitions(used_dataset.data.y[:smaller_dataset_len],\n",
    "                                            num_folds=5, valid_set_portion=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training graphs: \"+ str(len(fold_partitions[0]['train'])))\n",
    "print(\"Number of validation graphs: \"+ str(len(fold_partitions[0]['validation'])))\n",
    "print(\"Number of testing graphs: \"+ str(len(fold_partitions[0]['test'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = {\n",
    "    \"batch_size\" : 300,\n",
    "    \"num_epochs\" : 50,\n",
    "    \n",
    "    \"emb_dim\" : 300,\n",
    "    \"gnn_type\" : \"gatv2\",\n",
    "    \"num_layer\" : 5,\n",
    "    \"graph_pooling\" : \"mean\", #attention\n",
    "    \n",
    "    \"input_embed_dim\" : None,\n",
    "    \"gene_embed_dim\": 1,\n",
    "    \"num_attn_heads\" : 2,\n",
    "    \"num_transformer_units\" : 1,\n",
    "    \"p_dropout\" : 0.3,\n",
    "#     \"nonlin_func\" : nn.ReLU(),\n",
    "    \"mlp_embed_factor\" : 2,\n",
    "    \"pooling_mode\" : 'attn',\n",
    "    \"dist_opt\" : 'cosine',\n",
    "\n",
    "    \"base_lr\" : 3e-4, #3e-4\n",
    "    \"max_lr_mul\": 5,\n",
    "    \"l2_reg\" : 1e-5,\n",
    "    \"loss_w\" : 1.,\n",
    "    \"margin_v\" : 1.,\n",
    "\n",
    "    \"expression_dim\" : 64,\n",
    "    \"expression_input_size\" : 908,\n",
    "    \"exp_H1\" : 4096,\n",
    "    \"exp_H2\" : 2048\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_embed_dim = [128]\n",
    "# num_attn_heads = [2] # 2,4\n",
    "# num_transformer_units = [1]\n",
    "emb_dim = [100]\n",
    "num_layer = [5]\n",
    "p_dropout = [0.3]\n",
    "# nonlin_func = [nn.ReLU()]\n",
    "# mlp_embed_factor = [2]\n",
    "# pooling_mode = ['attn']\n",
    "# dist_opt = ['cosine']\n",
    "l2_reg = [1e-7] #0\n",
    "# batch_size = [300]\n",
    "# num_epochs = [200]\n",
    "# loss_w = [0.95] # 0.05, \n",
    "base_lr = [3e-4]\n",
    "max_lr_mul = [10]\n",
    "exp_H1 = [512, 1024, 2048, 4096]\n",
    "exp_H2 = [512, 1024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_names = [\"num_layer\",\n",
    "            \"p_dropout\",\n",
    "            \"l2_reg\",\n",
    "            \"emb_dim\",\n",
    "#             \"loss_w\",\n",
    "#            \"num_transformer_units\",\n",
    "#            \"batch_size\",\n",
    "           \"base_lr\",\n",
    "           \"max_lr_mul\",\n",
    "           \"exp_H1\",\n",
    "           \"exp_H2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[globals()[i] for i in hp_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_space = list(itertools.product(*[globals()[i] for i in hp_names]))\n",
    "print(len(hyperparam_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_space[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spawn_q_process(q_process):\n",
    "    print(\">>> spawning hyperparam search process\")\n",
    "    q_process.start()\n",
    "    \n",
    "def join_q_process(q_process):\n",
    "    q_process.join()\n",
    "    print(\"<<< joined hyperparam search process\")\n",
    "    \n",
    "def create_q_process(queue, used_dataset, gpu_num, tphp, exp_dir, partition): #\n",
    "    return mp.Process(target=deepadr.train_functions.run_exp, args=(queue, used_dataset, gpu_num, tphp, exp_dir, partition)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "queue = mp.Queue()\n",
    "q_processes = []\n",
    "\n",
    "partition = fold_partitions[0]\n",
    "\n",
    "print(\"Start: \" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "\n",
    "for q_i in range(min(n_gpu, len(hyperparam_space))):\n",
    "    time_stamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    exp_dir = create_directory(os.path.join(targetdata_dir_exp, \"exp_\"+str(q_i)+\"_\"+time_stamp))\n",
    "    create_directory(os.path.join(exp_dir, \"predictions\"))\n",
    "    tphp = generate_tp_hp(tp, hyperparam_space[q_i], hp_names)\n",
    "    \n",
    "    q_process = create_q_process(queue, used_dataset, q_i, tphp, exp_dir, partition)\n",
    "    q_processes.append(q_process)\n",
    "    spawn_q_process(q_process)\n",
    "\n",
    "spawned_processes = n_gpu\n",
    "    \n",
    "for q_i in range(len(hyperparam_space)):\n",
    "    join_q_process(q_processes[q_i])\n",
    "    released_gpu_num = queue.get()\n",
    "    print(\"released_gpu_num:\", released_gpu_num)\n",
    "    if(spawned_processes < len(hyperparam_space)):\n",
    "#         device_gpu = get_device(True, index=q_i)\n",
    "        time_stamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "        exp_dir = create_directory(os.path.join(targetdata_dir_exp, \"exp_\"+str(q_i)+\"_\"+time_stamp))\n",
    "        create_directory(os.path.join(exp_dir, \"predictions\"))\n",
    "        tphp = generate_tp_hp(tp, hyperparam_space[q_i], hp_names)\n",
    "\n",
    "        q_process = create_q_process(queue, used_dataset, released_gpu_num, tphp, exp_dir, partition)\n",
    "        q_processes.append(q_process)\n",
    "        spawn_q_process(q_process)\n",
    "        spawned_processes = spawned_processes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"End: \" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "exp_dirs = glob.glob(targetdata_dir_exp+\"/hyp/exp_*\")\n",
    "len(exp_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_results = []\n",
    "\n",
    "for edir in exp_dirs:\n",
    "    print(edir)\n",
    "    \n",
    "    hp = pd.read_json(edir + \"/hyperparameters.json\", typ=\"Series\").to_dict()\n",
    "    hp['maxTestAUPR'] = pd.read_csv(edir + \"/curves.csv\")['test_aupr'].max()\n",
    "    exp_results.append(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hip.Experiment.from_iterable(exp_results).display(force_full_width=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
